{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc1d360c",
   "metadata": {},
   "source": [
    "#Add this to Requiremtns \n",
    "+ faiss-cpu \n",
    "+ sentence-transformers \n",
    "+ langchain-huggingface"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8801275",
   "metadata": {},
   "source": [
    "\n",
    "1) Ingestion: You read the file.\n",
    "\n",
    "2) Splitting: You cut it into pieces.\n",
    "\n",
    "2) Embedding: The HuggingFaceEmbeddings model read your text and turned it into vectors (lists of numbers).\n",
    "\n",
    "4) Vector DB: FAISS organized these vectors in 3D space.\n",
    "\n",
    "5) Retrieval: When you asked about the \"purpose,\" it didn't look for the word \"purpose.\" It looked for the concept of motivation/reasoning in the vectors and returned the right paragraph.\n",
    "\n",
    "`Load -> Split -> Embed -> Store -> Search`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7f01ec9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "\n",
    "#Gemini\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "\n",
    "os.environ['GOOGLE_API_KEY'] = os.getenv(\"GOOGLE_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "41b3205d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The world must be made safe for democracy. Its peace must be planted upon the tested foundations of\n",
      "foundations of political liberty. We have no selfish ends to serve. We desire no conquest, no\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "49"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- STEP 1: Load & Split (Review) ---\n",
    "# Lets Load speech.txt and then split 100 and overlap 20\n",
    "\n",
    "#Load\n",
    "loader = TextLoader(\"./files/speech.txt\")\n",
    "docs = loader.load()\n",
    "docs\n",
    "\n",
    "#Split\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 100,\n",
    "    chunk_overlap = 20\n",
    ")\n",
    "\n",
    "chunks = splitter.split_documents(docs)\n",
    "print(chunks[0].page_content)\n",
    "print(chunks[1].page_content)\n",
    "len(chunks)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bb7e89f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\agentic2.0\\venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\manis\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    }
   ],
   "source": [
    "# --- STEP 2: The Embedding Model (The Translator) ---\n",
    "# We use a free, powerful model from HuggingFace\n",
    "\n",
    "embedding = HuggingFaceEmbeddings(model=\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "47c00b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- STEP 3: The Vector Store (The Database) ---\n",
    "# This line does the heavy lifting:\n",
    "# 1. Takes all chunks.\n",
    "# 2. Converts them to numbers using the embedding model.\n",
    "# 3. Stores them in a local FAISS index.\n",
    "\n",
    "vector_db = FAISS.from_documents(chunks,embedding)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e6fbd705",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It is a distressing and oppressive duty, gentlemen of the Congress, which I have performed in thus\n"
     ]
    }
   ],
   "source": [
    "# --- STEP 4: The Search (The Test) ---\n",
    "# Now we ask a question. The DB finds the most mathematically similar chunk.\n",
    "\n",
    "query = \"What is this speech about?\"\n",
    "\n",
    "# Search for the top 2 most relevant chunks\n",
    "\n",
    "results = vector_db.similarity_search(query,k=2)\n",
    "\n",
    "print(results[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bbafb3b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Just because we fight without rancor and without selfish object, seeking nothing for ourselves but what we shall wish to share with all free peoples, we shall, I feel confident, conduct our\n"
     ]
    }
   ],
   "source": [
    "# Lets play with chunk_size and overlap and find the best size\n",
    "\n",
    "# Split data is in docs\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 200,\n",
    "    chunk_overlap = 40,\n",
    ")\n",
    "chunks = splitter.split_documents(docs)\n",
    "\n",
    "# Embedding\n",
    "embeddings = HuggingFaceEmbeddings(model='all-MiniLM-L6-v2')\n",
    "\n",
    "# Vectordb\n",
    "vector_db = FAISS.from_documents(chunks,embeddings)\n",
    "\n",
    "#Test and search\n",
    "query = \"What is the motive of Speech\"\n",
    "result = vector_db.similarity_search(query,k=2)\n",
    "\n",
    "print(result[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83260014",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
